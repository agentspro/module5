"""
RAG AGENT - LangGraph 1.0 –∑ Checkpointing
–ù–∞ –±–∞–∑—ñ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü—ñ—ó: LangGraph Agentic RAG (2025)

Implements:
- Agentic RAG pattern –∑ dynamic retrieval
- LangGraph StateGraph –¥–ª—è orchestration
- Checkpointing –∑ MemorySaver –¥–ª—è persistence
- Multi-step reasoning: retrieve ‚Üí grade ‚Üí generate
- LangSmith automatic tracing

Pattern:
User Query ‚Üí Retrieve Docs ‚Üí Grade Relevance ‚Üí Generate Answer
          ‚Üì (if irrelevant)
       Rewrite Query ‚Üí Retrieve Again
"""

import os
from typing import TypedDict, Annotated, List, Dict
from operator import add
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_core.documents import Document
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_community.vectorstores import FAISS
from langgraph.graph import StateGraph, END
from langgraph.checkpoint.memory import MemorySaver
from pydantic import BaseModel, Field
# Updated import: deprecated TavilySearchResults replaced by TavilySearch in langchain-tavily package
try:
    from langchain_tavily import TavilySearch
except ImportError:
    TavilySearch = None  # Graceful fallback if lib not installed

from dotenv import load_dotenv


load_dotenv()

# ============================================================================
# LANGSMITH SETUP
# ============================================================================

if os.getenv("LANGCHAIN_TRACING_V2") == "true":
    print("OK LangSmith —Ç—Ä–µ–π—Å–∏–Ω–≥ –∞–∫—Ç–∏–≤–Ω–∏–π –¥–ª—è RAG Agent")
    print(f"Stats: Project: {os.getenv('LANGCHAIN_PROJECT', 'default')}")
    print("üîç All RAG operations will be traced: retrieve ‚Üí grade ‚Üí generate\n")
else:
    print("WARNING  LangSmith –Ω–µ –≤–≤—ñ–º–∫–Ω–µ–Ω–∏–π\n")


# ============================================================================
# KNOWLEDGE BASE - –°—Ç–≤–æ—Ä—é—î–º–æ –≤–µ–∫—Ç–æ—Ä–Ω—É –±–∞–∑—É –¥–ª—è RAG
# ============================================================================

def create_knowledge_base():
    """
    –°—Ç–≤–æ—Ä—é—î –≤–µ–∫—Ç–æ—Ä–Ω—É –±–∞–∑—É –∑–Ω–∞–Ω—å –∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏

    Returns:
        FAISS retriever for similarity search
    """

    documents = [
        Document(
            page_content="""
            LangChain 1.0 Release (October 2025):
            LangChain 1.0 introduces stable APIs with create_agent function.
            No breaking changes until 2.0. Key features: middleware architecture,
            improved observability, production-ready patterns.
            """,
            metadata={"source": "release_notes", "topic": "langchain", "date": "2025-10"}
        ),
        Document(
            page_content="""
            LangGraph 1.0 Checkpointing:
            LangGraph provides checkpointing using MemorySaver for development
            and PostgresSaver for production. Checkpoints allow agents to pause,
            resume, and time-travel through execution states. Thread_id enables
            separate conversation sessions.
            """,
            metadata={"source": "docs", "topic": "langgraph", "feature": "checkpointing"}
        ),
        Document(
            page_content="""
            Agent Middleware (New in 2025):
            LangChain 1.0 middleware provides three hooks:
            - before_model: Pre-processing before LLM calls
            - after_model: Post-processing after LLM responds
            - modify_model_request: Modify tools, prompts, messages
            Built-in middlewares: Human-in-the-loop, Summarization, PII redaction.
            """,
            metadata={"source": "docs", "topic": "middleware", "version": "1.0"}
        ),
        Document(
            page_content="""
            Agentic RAG Pattern:
            Agentic RAG uses AI agents to dynamically manage retrieval strategies.
            Pattern: Query ‚Üí Retrieve ‚Üí Grade ‚Üí (if bad) Rewrite ‚Üí Retrieve Again.
            Agents can iteratively refine context, route to web search if needed,
            and orchestrate multiple retrieval sources.
            """,
            metadata={"source": "patterns", "topic": "rag", "type": "agentic"}
        ),
        Document(
            page_content="""
            LangSmith Integration:
            Set LANGCHAIN_TRACING_V2=true to enable automatic tracing.
            LangSmith provides observability for agents: trace visualization,
            cost tracking, latency analysis, and A/B testing capabilities.
            Works with LangChain, LangGraph, and custom agents.
            """,
            metadata={"source": "docs", "topic": "observability", "tool": "langsmith"}
        ),
        Document(
            page_content="""
            Production Best Practices:
            - Use create_agent instead of deprecated AgentExecutor
            - Implement error handling and max_iterations
            - Add middleware for security and cost control
            - Use checkpointing for long-running agents
            - Monitor with LangSmith in production
            - Pin dependency versions for stability
            """,
            metadata={"source": "best_practices", "topic": "production"}
        )
    ]

    print("KB: Creating knowledge base with documents:")
    for i, doc in enumerate(documents, 1):
        print(f"  {i}. {doc.metadata.get('topic', 'general')}: {doc.page_content[:50]}...")

    # Create embeddings and vector store
    embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
    vectorstore = FAISS.from_documents(documents, embeddings)

    retriever = vectorstore.as_retriever(
        search_type="similarity",
        search_kwargs={"k": 2}  # Return top 2 most relevant docs
    )

    print(f"OK Knowledge base created with {len(documents)} documents\n")

    return retriever


# ============================================================================
# AGENTIC RAG STATE
# ============================================================================

class RAGState(TypedDict):
    """
    State –¥–ª—è Agentic RAG workflow

    Flow:
    question ‚Üí retrieved_docs ‚Üí relevance_grade ‚Üí answer
    """
    question: str                          # User's question
    retrieved_docs: List[Document]         # Retrieved documents
    relevance_grade: str                   # "relevant" or "irrelevant"
    rewrite_count: int                     # How many times we rewrote query
    answer: str                            # Final answer
    reasoning: Annotated[List[str], add]  # Reasoning steps (accumulated)


# ============================================================================
# AGENTIC RAG NODES
# ============================================================================

def retrieve_documents(state: RAGState) -> RAGState:
    """
    Node 1: Retrieve documents based on question
    """

    print(f"\n{'='*60}")
    print("KB: NODE: Retrieve Documents")
    print(f"{'='*60}")

    question = state["question"]
    print(f"Question: {question}")

    # Get retriever from global scope (in production pass via state or config)
    docs = GLOBAL_RETRIEVER.invoke(question)

    print(f"\nRetrieved {len(docs)} documents:")
    for i, doc in enumerate(docs, 1):
        print(f"  {i}. Topic: {doc.metadata.get('topic', 'N/A')}")
        print(f"     Content: {doc.page_content[:80]}...")

    reasoning = [f"Retrieved {len(docs)} documents for query"]

    return {
        **state,
        "retrieved_docs": docs,
        "reasoning": reasoning
    }


def grade_documents(state: RAGState) -> RAGState:
    """
    Node 2: Grade relevance of retrieved documents
    Uses LLM to determine if docs are relevant to question
    """

    print(f"\n{'='*60}")
    print("‚öñÔ∏è  NODE: Grade Document Relevance")
    print(f"{'='*60}")

    question = state["question"]
    docs = state["retrieved_docs"]

    # Structured output –¥–ª—è grading
    class GradeOutput(BaseModel):
        """Grade output schema"""
        relevance: str = Field(description="'relevant' or 'irrelevant'")
        reasoning: str = Field(description="Why this grade")

    llm = ChatOpenAI(model="gpt-4o", temperature=0)
    structured_llm = llm.with_structured_output(GradeOutput)

    grade_prompt = ChatPromptTemplate.from_messages([
        ("system", "You are a grader. Assess if retrieved documents are relevant to the question."),
        ("human", """Question: {question}

Retrieved Documents:
{documents}

Are these documents relevant to answer the question?
Respond with 'relevant' or 'irrelevant' and explain why.""")
    ])

    # Format documents
    docs_text = "\n\n".join([
        f"Doc {i+1}: {doc.page_content}"
        for i, doc in enumerate(docs)
    ])

    # Grade
    chain = grade_prompt | structured_llm
    grade_result = chain.invoke({
        "question": question,
        "documents": docs_text
    })

    # Handle both Pydantic model instance and potential dict output for robustness
    if hasattr(grade_result, "relevance"):
        relevance = grade_result.relevance # type: ignore
        grade_reasoning = grade_result.reasoning # type: ignore
    else:
        relevance = grade_result["relevance"]
        grade_reasoning = grade_result["reasoning"]

    print(f"\nGrade: {relevance.upper()}")
    print(f"Reasoning: {grade_reasoning}")

    reasoning = [f"Graded documents as {relevance}: {grade_reasoning}"]

    return {
        **state,
        "relevance_grade": relevance,
        "reasoning": reasoning
    }


def rewrite_query(state: RAGState) -> RAGState:
    """
    Node 3: Rewrite query if documents were irrelevant
    Uses LLM to reformulate question for better retrieval
    """

    print(f"\n{'='*60}")
    print("‚úçÔ∏è  NODE: Rewrite Query")
    print(f"{'='*60}")

    original_question = state["question"]
    rewrite_count = state.get("rewrite_count", 0)

    print(f"Original question: {original_question}")
    print(f"Rewrite attempt: {rewrite_count + 1}")

    llm = ChatOpenAI(model="gpt-4o", temperature=0.7)

    rewrite_prompt = ChatPromptTemplate.from_messages([
        ("system", "You are a query rewriter. Improve the question to get better search results."),
        ("human", """Original question: {question}

The retrieved documents were not relevant. Rewrite this question to:
1. Be more specific
2. Include key technical terms
3. Focus on the core information need

Return ONLY the rewritten question, nothing else.""")
    ])

    chain = rewrite_prompt | llm | StrOutputParser()
    new_question = chain.invoke({"question": original_question})

    print(f"Rewritten question: {new_question}\n")

    reasoning = [f"Rewrote query (attempt {rewrite_count + 1}): '{new_question}'"]

    return {
        **state,
        "question": new_question,
        "rewrite_count": rewrite_count + 1,
        "reasoning": reasoning
    }


def generate_answer(state: RAGState) -> RAGState:
    """
    Node 4: Generate final answer using retrieved documents
    """

    print(f"\n{'='*60}")
    print("TIP: NODE: Generate Answer")
    print(f"{'='*60}")

    question = state["question"]
    docs = state["retrieved_docs"]

    llm = ChatOpenAI(model="gpt-4o", temperature=0)

    rag_prompt = ChatPromptTemplate.from_messages([
        ("system", """You are a helpful AI assistant. Answer questions based on the provided context.

Context:
{context}

Question: {question}

Instructions:
- Use ONLY information from the context
- Be concise and accurate
- If context doesn't contain the answer, say so
- Include relevant details and examples"""),
    ])

    # Format context
    context = "\n\n".join([
        f"Source {i+1} ({doc.metadata.get('topic', 'N/A')}):\n{doc.page_content}"
        for i, doc in enumerate(docs)
    ])

    chain = rag_prompt | llm | StrOutputParser()
    answer = chain.invoke({
        "context": context,
        "question": question
    })

    print(f"Generated answer ({len(answer)} chars)")

    reasoning = [f"Generated final answer using {len(docs)} documents"]

    return {
        **state,
        "answer": answer,
        "reasoning": reasoning
    }


# ============================================================================
# ROUTING LOGIC
# ============================================================================

def decide_next_step(state: RAGState) -> str:
    """
    Conditional edge: Decide whether to generate or rewrite

    Logic:
    - If docs are relevant ‚Üí generate answer
    - If docs are irrelevant AND we haven't rewritten too many times ‚Üí rewrite
    - If we've rewritten too much ‚Üí generate anyway (best effort)
    """

    relevance = state.get("relevance_grade", "irrelevant")
    rewrite_count = state.get("rewrite_count", 0)
    max_rewrites = 2

    print(f"\nü§î ROUTING: relevance={relevance}, rewrites={rewrite_count}")

    # If relevant ‚Üí generate
    if relevance == "relevant":
        print("   ‚Üí Going to: generate_answer (docs are good)")
        return "generate"

    # If still irrelevant and we have remaining rewrites ‚Üí rewrite
    if relevance == "irrelevant" and rewrite_count < max_rewrites:
        print(f"   ‚Üí Going to: rewrite_query (try {rewrite_count + 1}/{max_rewrites})")
        return "rewrite"

    # If we've hit max rewrites and still irrelevant ‚Üí web search fallback
    if relevance == "irrelevant" and rewrite_count >= max_rewrites:
        print("   ‚Üí Going to: web_search_fallback (exhausted rewrites)")
        return "web_search"

    # Default safety
    print("   ‚Üí Defaulting to generate_answer")
    return "generate"


def web_search_fallback(state: RAGState) -> RAGState:
    """
    Fallback Node: Perform web search via Tavily when local KB retrieval failed
    after allowed rewrites.

    Converts Tavily search results to Document objects and replaces retrieved_docs.
    """

    print(f"\n{'='*60}")
    print("üåê  NODE: Web Search Fallback (Tavily)")
    print(f"{'='*60}")

    question = state["question"]
    api_key = os.getenv("TAVILY_API_KEY")

    if not api_key:
        print("WARNING  TAVILY_API_KEY not found. Returning empty docs.")
        reasoning = ["Web search skipped: missing TAVILY_API_KEY"]
        return {
            **state,
            "retrieved_docs": [],
            "relevance_grade": "fallback_no_api",
            "reasoning": reasoning
        }

    try:
        if TavilySearch is None:
            reasoning = ["ERROR: langchain-tavily not installed. Run: pip install -U langchain-tavily"]
            return {
                **state,
                "retrieved_docs": [],
                "relevance_grade": "fallback_no_tool",
                "reasoning": reasoning
            }

        # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –æ–Ω–æ–≤–ª–µ–Ω–∏–π TavilySearch tool (langchain-tavily)
        search_tool = TavilySearch(max_results=3, api_key=api_key)

        raw = search_tool.invoke({"query": question})

        # Normalize different possible return shapes
        results = []
        if raw:
            if isinstance(raw, dict):
                # Some versions return {'results': [...], 'query': '...'}
                if 'results' in raw and isinstance(raw['results'], list):
                    results = raw['results']
                else:
                    # Treat dict itself as single result
                    results = [raw]
            elif isinstance(raw, list):
                results = raw
            else:  # string or other primitive
                results = [raw]

        docs: List[Document] = []
        for r in results:
            if isinstance(r, dict):
                title = r.get("title") or r.get("name") or "Untitled"
                url = r.get("url", "")
                content = r.get("content") or r.get("text") or r.get("snippet") or "No content"
                page_text = f"{title}\n{content}\nSource: {url}".strip()
                docs.append(Document(page_content=page_text, metadata={"source": "tavily", "url": url}))
            else:
                # Fallback if element is a plain string
                docs.append(Document(page_content=str(r), metadata={"source": "tavily"}))

        if not docs:
            print("Tavily returned no usable results")
            reasoning = ["Web search fallback returned no results"]
            return {
                **state,
                "retrieved_docs": [],
                "relevance_grade": "web_empty",
                "reasoning": reasoning
            }

        print(f"Retrieved {len(docs)} web documents from Tavily")
        for i, d in enumerate(docs, 1):
            print(f"  {i}. {d.page_content[:90]}...")

        reasoning = [f"Performed web search fallback: {len(docs)} web docs"]

        return {
            **state,
            "retrieved_docs": docs,
            "relevance_grade": "web_fallback",
            "reasoning": reasoning
        }
    except Exception as e:
        print(f"ERROR Tavily search failed: {e}")
        reasoning = [f"Web search error: {e}"]
        return {
            **state,
            "retrieved_docs": [],
            "relevance_grade": "web_error",
            "reasoning": reasoning
        }


# ============================================================================
# BUILD LANGGRAPH
# ============================================================================

def create_rag_agent():
    """
    –°—Ç–≤–æ—Ä—é—î Agentic RAG workflow –∑ LangGraph

    Graph structure:
    START ‚Üí retrieve ‚Üí grade ‚Üí [relevant? ‚Üí generate | irrelevant? ‚Üí rewrite ‚Üí retrieve]
    """

    print("=" * 70)
    print("Retry: BUILDING AGENTIC RAG GRAPH")
    print("=" * 70 + "\n")

    # Create graph
    workflow = StateGraph(RAGState)

    # Add nodes
    workflow.add_node("retrieve", retrieve_documents)
    workflow.add_node("grade", grade_documents)
    workflow.add_node("rewrite", rewrite_query)
    workflow.add_node("generate", generate_answer)
    workflow.add_node("web_search_fallback", web_search_fallback)

    # Build graph structure
    workflow.set_entry_point("retrieve")
    workflow.add_edge("retrieve", "grade")

    # Conditional routing from grade
    workflow.add_conditional_edges(
        "grade",
        decide_next_step,
        {
            "generate": "generate",
            "rewrite": "rewrite",
            "web_search": "web_search_fallback"
        }
    )

    # After rewrite, go back to retrieve
    workflow.add_edge("rewrite", "retrieve")

    # After web search fallback, go directly to generate
    workflow.add_edge("web_search_fallback", "generate")

    # Generate is end
    workflow.add_edge("generate", END)

    # Compile with checkpointer for persistence
    checkpointer = MemorySaver()
    app = workflow.compile(checkpointer=checkpointer)

    print("Graph structure:")
    print("  START ‚Üí retrieve ‚Üí grade ‚Üí [decide]")
    print("                      ‚Üì")
    print("             if relevant: generate ‚Üí END")
    print("             if irrelevant & rewrites <2: rewrite ‚Üí retrieve (loop)")
    print("             if irrelevant & rewrites >=2: web_search_fallback ‚Üí generate ‚Üí END")
    print()
    print("OK RAG Agent compiled with MemorySaver checkpointer\n")

    return app


# ============================================================================
# TESTING
# ============================================================================

def test_rag_agent():
    """Test Agentic RAG with different queries"""

    global GLOBAL_RETRIEVER
    GLOBAL_RETRIEVER = create_knowledge_base()

    agent = create_rag_agent()

    test_queries = [
        {
            "question": "What are the new middleware hooks in LangChain 1.0?",
            "expected": "Should find info about before_model, after_model, modify_model_request"
        },
        {
            "question": "How does checkpointing work in LangGraph?",
            "expected": "Should find info about MemorySaver and thread_id"
        },
        {
            "question": "What is the capital of France?",
            "expected": "Should say not in context (trigger rewrite?)"
        }
    ]

    for i, test in enumerate(test_queries, 1):
        print("\n" + "=" * 70)
        print(f"TEST {i}: {test['question']}")
        print(f"Expected: {test['expected']}")
        print("=" * 70)

        # Use unique thread_id for each query to maintain separate sessions
        config = {"thread_id": f"test_query_{i}"}

        initial_state: RAGState = {
            "question": test["question"],
            "retrieved_docs": [],
            "relevance_grade": "",
            "rewrite_count": 0,
            "answer": "",
            "reasoning": []
        }

        try:
            result = agent.invoke(initial_state, config) # type: ignore

            print(f"\n{'='*70}")
            print("FINAL RESULT:")
            print(f"{'='*70}")
            print(f"\nLOG Question: {result['question']}")
            print(f"\nTIP: Answer:\n{result['answer']}")
            print(f"\nüîç Reasoning steps:")
            for j, step in enumerate(result['reasoning'], 1):
                print(f"  {j}. {step}")
            print(f"\nStats: Stats:")
            print(f"  - Query rewrites: {result['rewrite_count']}")
            print(f"  - Documents used: {len(result['retrieved_docs'])}")
            print(f"  - Final grade: {result['relevance_grade']}")

        except Exception as e:
            print(f"\nERROR: Error: {e}")
            import traceback
            traceback.print_exc()

        if i < len(test_queries):
            input("\nPAUSE  Press Enter for next test...\n")


# ============================================================================
# MAIN
# ============================================================================

if __name__ == "__main__":
    print("\nTARGET LangGraph 1.0 - Agentic RAG with Checkpointing")
    print("=" * 70)
    print("\nFeatures:")
    print("  OK Dynamic retrieval with relevance grading")
    print("  OK Automatic query rewriting if docs irrelevant")
    print("  OK LangGraph StateGraph for orchestration")
    print("  OK MemorySaver checkpointing for persistence")
    print("  OK Conditional routing based on relevance")
    print("  OK LangSmith tracing for full observability")
    print("\n" + "=" * 70 + "\n")

    if not os.getenv("OPENAI_API_KEY"):
        print("ERROR: ERROR: OPENAI_API_KEY not found!")
        exit(1)

    try:
        test_rag_agent()

        print("\n" + "=" * 70)
        print("OK ALL RAG TESTS COMPLETED")
        print("=" * 70)
        print("\nTIP: Agentic RAG Pattern Benefits:")
        print("  ‚Ä¢ Dynamic retrieval strategy")
        print("  ‚Ä¢ Self-correction through query rewriting")
        print("  ‚Ä¢ Relevance grading prevents hallucination")
        print("  ‚Ä¢ Checkpointing enables pause/resume")
        print("\nTIP: Check LangSmith for full execution traces!\n")

    except KeyboardInterrupt:
        print("\n\n‚èπÔ∏è  Interrupted")
    except Exception as e:
        print(f"\nERROR: Error: {e}")
        import traceback
        traceback.print_exc()
